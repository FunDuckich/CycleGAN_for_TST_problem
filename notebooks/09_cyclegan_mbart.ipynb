{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ea8a87-c4fb-4641-84fb-6e611c0ca0de",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:02.874141Z",
          "iopub.status.busy": "2025-05-07T07:34:02.873809Z",
          "iopub.status.idle": "2025-05-07T07:34:24.928851Z",
          "shell.execute_reply": "2025-05-07T07:34:24.927954Z",
          "shell.execute_reply.started": "2025-05-07T07:34:02.874118Z"
        },
        "tags": [],
        "id": "84ea8a87-c4fb-4641-84fb-6e611c0ca0de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration, MBart50TokenizerFast,\n",
        "    DistilBertModel, DistilBertTokenizer,\n",
        "    get_linear_schedule_with_warmup, GenerationConfig\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc6702c-b388-4bb6-a348-e3386098ef9b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:24.930906Z",
          "iopub.status.busy": "2025-05-07T07:34:24.930349Z",
          "iopub.status.idle": "2025-05-07T07:34:25.128863Z",
          "shell.execute_reply": "2025-05-07T07:34:25.127996Z",
          "shell.execute_reply.started": "2025-05-07T07:34:24.930884Z"
        },
        "tags": [],
        "id": "ebc6702c-b388-4bb6-a348-e3386098ef9b",
        "outputId": "eb1aedc4-0615-4e45-b127-e45e490b3298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "TAG_TO_LIT = \"[TO_LIT]\"\n",
        "TAG_TO_CONV = \"[TO_CONV]\"\n",
        "\n",
        "GEN_MODEL_NAME = \"sn4kebyt3/ru-bart-large\"\n",
        "CLASSIFIER_MODEL_NAME = \"DeepPavlov/distilrubert-base-cased-conversational\"\n",
        "MAX_LENGTH = 128\n",
        "NUM_EPOCHS = 12\n",
        "\n",
        "BATCH_SIZE = 96\n",
        "STEPS_PER_EPOCH = 125\n",
        "\n",
        "LEARNING_RATE_GEN = 2e-5\n",
        "LEARNING_RATE_DISC = 4e-5\n",
        "TEST_SIZE = 0.1\n",
        "VAL_TEST_SPLIT = 0.5\n",
        "RANDOM_STATE = 42\n",
        "SAVE_DIR = \"tag_cyclegan_bart_final_v2\"\n",
        "MODEL_PATH_G_BEST = os.path.join(SAVE_DIR, \"G_bart_best.pth\")\n",
        "MODEL_PATH_D_C_BEST = os.path.join(SAVE_DIR, \"D_C_best.pth\")\n",
        "MODEL_PATH_D_L_BEST = os.path.join(SAVE_DIR, \"D_L_best.pth\")\n",
        "PLOT_PATH = os.path.join(SAVE_DIR, \"training_plots_final.png\")\n",
        "\n",
        "LAMBDA_CYCLE = 4.0\n",
        "LAMBDA_IDENTITY = 4.0\n",
        "LAMBDA_STYLE = 12.0\n",
        "LAMBDA_ADV = 1.5\n",
        "\n",
        "base_gen_config_train = GenerationConfig(\n",
        "    max_length=MAX_LENGTH, min_length=5, num_beams=3,\n",
        "    early_stopping=True, temperature=1.0,\n",
        "    repetition_penalty=1.0, no_repeat_ngram_size=0\n",
        ")\n",
        "base_gen_config_val_log = GenerationConfig(\n",
        "    max_length=MAX_LENGTH, min_length=10, num_beams=3,\n",
        "    early_stopping=True, temperature=1.0,\n",
        "    repetition_penalty=1.2, no_repeat_ngram_size=4\n",
        ")\n",
        "NUM_VAL_STYLE_EXAMPLES = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "248287fe-ef9e-42ca-b521-b9f5cd207af4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:25.130244Z",
          "iopub.status.busy": "2025-05-07T07:34:25.129869Z",
          "iopub.status.idle": "2025-05-07T07:34:34.498948Z",
          "shell.execute_reply": "2025-05-07T07:34:34.498079Z",
          "shell.execute_reply.started": "2025-05-07T07:34:25.130221Z"
        },
        "tags": [],
        "id": "248287fe-ef9e-42ca-b521-b9f5cd207af4",
        "outputId": "1cb7969d-3fcf-4cbd-800b-116d727fd5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загружено 1382549 разговорных и 1382549 литературных текстов.\n",
            "Размеры датасетов: Train C: 1244294, Val C: 69127\n",
            "                   Train L: 1244294, Val L: 69127\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    data = pd.read_csv(\"data.csv\")\n",
        "    texts_C_full = list(data['tg_text'].astype(str))\n",
        "    texts_L_full = list(data['lit_text'].astype(str))\n",
        "    print(f\"Загружено {len(texts_C_full)} разговорных и {len(texts_L_full)} литературных текстов.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка загрузки данных: {e}\"); raise\n",
        "\n",
        "train_texts_C, temp_texts_C = train_test_split(texts_C_full, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "train_texts_L, temp_texts_L = train_test_split(texts_L_full, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "val_texts_C, _ = train_test_split(temp_texts_C, test_size=VAL_TEST_SPLIT, random_state=RANDOM_STATE)\n",
        "val_texts_L, _ = train_test_split(temp_texts_L, test_size=VAL_TEST_SPLIT, random_state=RANDOM_STATE)\n",
        "print(f\"Размеры датасетов: Train C: {len(train_texts_C)}, Val C: {len(val_texts_C)}\")\n",
        "print(f\"                   Train L: {len(train_texts_L)}, Val L: {len(val_texts_L)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eff685e-bb4c-4557-a0d5-ae4f6f6d6f06",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:34.501054Z",
          "iopub.status.busy": "2025-05-07T07:34:34.500632Z",
          "iopub.status.idle": "2025-05-07T07:34:35.754477Z",
          "shell.execute_reply": "2025-05-07T07:34:35.753584Z",
          "shell.execute_reply.started": "2025-05-07T07:34:34.501019Z"
        },
        "tags": [],
        "id": "3eff685e-bb4c-4557-a0d5-ae4f6f6d6f06",
        "outputId": "f5a32199-7897-4552-af06-3b3953922ded"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Добавлено 2 спец. токенов в gen_tokenizer: [TO_LIT], [TO_CONV] (Новый размер словаря: 24263)\n",
            "ID токена русского языка для mBART генератора: 24228\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    gen_tokenizer = MBart50TokenizerFast.from_pretrained(GEN_MODEL_NAME)\n",
        "    style_tokenizer = DistilBertTokenizer.from_pretrained(CLASSIFIER_MODEL_NAME)\n",
        "\n",
        "    special_tokens_to_add = {'additional_special_tokens': [TAG_TO_LIT, TAG_TO_CONV]}\n",
        "    num_added_toks = gen_tokenizer.add_special_tokens(special_tokens_to_add)\n",
        "    print(f\"Добавлено {num_added_toks} спец. токенов в gen_tokenizer: {TAG_TO_LIT}, {TAG_TO_CONV} (Новый размер словаря: {len(gen_tokenizer)})\")\n",
        "\n",
        "    RUSSIAN_TOKEN_ID = gen_tokenizer.lang_code_to_id.get(\"ru_RU\", gen_tokenizer.eos_token_id)\n",
        "    if RUSSIAN_TOKEN_ID == gen_tokenizer.eos_token_id and \"ru_RU\" not in gen_tokenizer.lang_code_to_id:\n",
        "        print(f\"ПРЕДУПРЕЖДЕНИЕ: Токен языка 'ru_RU' не найден. Используется eos_token_id ({RUSSIAN_TOKEN_ID})\")\n",
        "    print(f\"ID токена русского языка для mBART генератора: {RUSSIAN_TOKEN_ID}\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка инициализации токенизаторов: {e}\"); raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636d134c-38cf-4f41-b20b-e78bc13b68ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:35.755731Z",
          "iopub.status.busy": "2025-05-07T07:34:35.755300Z",
          "iopub.status.idle": "2025-05-07T07:34:35.782504Z",
          "shell.execute_reply": "2025-05-07T07:34:35.781873Z",
          "shell.execute_reply.started": "2025-05-07T07:34:35.755709Z"
        },
        "tags": [],
        "id": "636d134c-38cf-4f41-b20b-e78bc13b68ff"
      },
      "outputs": [],
      "source": [
        "def update_generation_config(base_config, model_config_obj, tokenizer_pad_token_id):\n",
        "    updated_config = GenerationConfig.from_dict(base_config.to_dict())\n",
        "    updated_config.decoder_start_token_id = model_config_obj.decoder_start_token_id\n",
        "    updated_config.eos_token_id = model_config_obj.eos_token_id\n",
        "    updated_config.pad_token_id = tokenizer_pad_token_id\n",
        "    if hasattr(model_config_obj, 'forced_bos_token_id') and model_config_obj.forced_bos_token_id is not None:\n",
        "        updated_config.forced_bos_token_id = model_config_obj.forced_bos_token_id\n",
        "    return updated_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "105f1861-6dee-4dc5-8554-981651e5ff01",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:35.783546Z",
          "iopub.status.busy": "2025-05-07T07:34:35.783224Z",
          "iopub.status.idle": "2025-05-07T07:34:35.796351Z",
          "shell.execute_reply": "2025-05-07T07:34:35.795717Z",
          "shell.execute_reply.started": "2025-05-07T07:34:35.783526Z"
        },
        "tags": [],
        "id": "105f1861-6dee-4dc5-8554-981651e5ff01"
      },
      "outputs": [],
      "source": [
        "class StyleDataset(Dataset):\n",
        "    def __init__(self, texts_list, generator_tokenizer, max_sequence_length,\n",
        "                 generator_input_tag=None, create_identity_labels=False):\n",
        "        self.texts = [str(text) for text in texts_list]\n",
        "        self.generator_tokenizer = generator_tokenizer\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.generator_input_tag = generator_input_tag\n",
        "        self.create_identity_labels = create_identity_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_text_string = self.texts[idx]\n",
        "        input_text_for_g = f\"{self.generator_input_tag} {original_text_string}\" if self.generator_input_tag else original_text_string\n",
        "        tokenized_g_input = self.generator_tokenizer(input_text_for_g, padding=\"max_length\", truncation=True, max_length=self.max_sequence_length, return_tensors='pt')\n",
        "        item = {key: val.squeeze(0) for key, val in tokenized_g_input.items()}\n",
        "        item['original_text_str'] = original_text_string\n",
        "        tokenized_original = self.generator_tokenizer(original_text_string, padding=\"max_length\", truncation=True, max_length=self.max_sequence_length, return_tensors='pt')\n",
        "        item['original_ids'] = tokenized_original['input_ids'].squeeze(0)\n",
        "        item['original_mask'] = tokenized_original['attention_mask'].squeeze(0)\n",
        "        if self.create_identity_labels: item['labels'] = item['original_ids'].clone()\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f40fb5-b725-40f1-bf1f-2e20df063e66",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:35.797378Z",
          "iopub.status.busy": "2025-05-07T07:34:35.797034Z",
          "iopub.status.idle": "2025-05-07T07:34:36.387584Z",
          "shell.execute_reply": "2025-05-07T07:34:36.386828Z",
          "shell.execute_reply.started": "2025-05-07T07:34:35.797358Z"
        },
        "tags": [],
        "id": "60f40fb5-b725-40f1-bf1f-2e20df063e66"
      },
      "outputs": [],
      "source": [
        "train_dataset_C_main = StyleDataset(train_texts_C, gen_tokenizer, MAX_LENGTH)\n",
        "train_dataset_L_main = StyleDataset(train_texts_L, gen_tokenizer, MAX_LENGTH)\n",
        "train_dataloader_C = DataLoader(train_dataset_C_main, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0, pin_memory=True if device.type == 'cuda' else False)\n",
        "train_dataloader_L = DataLoader(train_dataset_L_main, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0, pin_memory=True if device.type == 'cuda' else False)\n",
        "\n",
        "val_dataset_L_identity_main = StyleDataset(val_texts_L, gen_tokenizer, MAX_LENGTH, generator_input_tag=TAG_TO_LIT, create_identity_labels=True)\n",
        "val_dataset_C_identity_main = StyleDataset(val_texts_C, gen_tokenizer, MAX_LENGTH, generator_input_tag=TAG_TO_CONV, create_identity_labels=True)\n",
        "val_dataloader_L_identity = DataLoader(val_dataset_L_identity_main, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True if device.type == 'cuda' else False)\n",
        "val_dataloader_C_identity = DataLoader(val_dataset_C_identity_main, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True if device.type == 'cuda' else False)\n",
        "\n",
        "val_dataset_C_for_style = StyleDataset(val_texts_C, gen_tokenizer, MAX_LENGTH)\n",
        "val_dataset_L_for_style = StyleDataset(val_texts_L, gen_tokenizer, MAX_LENGTH)\n",
        "val_dataloader_C_for_style = DataLoader(val_dataset_C_for_style, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True if device.type == 'cuda' else False)\n",
        "val_dataloader_L_for_style = DataLoader(val_dataset_L_for_style, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True if device.type == 'cuda' else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e560b82-3cd8-4701-9469-69bdd7df3ec7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:36.388797Z",
          "iopub.status.busy": "2025-05-07T07:34:36.388448Z",
          "iopub.status.idle": "2025-05-07T07:34:36.405893Z",
          "shell.execute_reply": "2025-05-07T07:34:36.405270Z",
          "shell.execute_reply.started": "2025-05-07T07:34:36.388776Z"
        },
        "tags": [],
        "id": "7e560b82-3cd8-4701-9469-69bdd7df3ec7"
      },
      "outputs": [],
      "source": [
        "# --- Модели ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, model_name_str, tokenizer_vocabulary_size, russian_language_token_id):\n",
        "        super().__init__()\n",
        "        self.bart_model = MBartForConditionalGeneration.from_pretrained(model_name_str)\n",
        "        self.bart_model.resize_token_embeddings(tokenizer_vocabulary_size)\n",
        "        self.generation_config_internal = self.bart_model.config\n",
        "        self.generation_config_internal.forced_bos_token_id = russian_language_token_id\n",
        "        self.generation_config_internal.decoder_start_token_id = russian_language_token_id\n",
        "        self.pad_token_id = gen_tokenizer.pad_token_id\n",
        "        print(f\"Генератор инициализирован: forced_bos={self.generation_config_internal.forced_bos_token_id}, dec_start={self.generation_config_internal.decoder_start_token_id}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        return self.bart_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    def generate_texts(self, input_ids, attention_mask, external_generation_config):\n",
        "        return self.bart_model.generate(input_ids=input_ids, attention_mask=attention_mask, generation_config=external_generation_config)\n",
        "\n",
        "    def get_attention_mask_for_generated(self, generated_ids_tensor):\n",
        "        return (generated_ids_tensor != self.pad_token_id).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f27481d-c8ac-4faf-a723-551bb81fd525",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:36.406897Z",
          "iopub.status.busy": "2025-05-07T07:34:36.406586Z",
          "iopub.status.idle": "2025-05-07T07:34:36.428426Z",
          "shell.execute_reply": "2025-05-07T07:34:36.427759Z",
          "shell.execute_reply.started": "2025-05-07T07:34:36.406878Z"
        },
        "tags": [],
        "id": "0f27481d-c8ac-4faf-a723-551bb81fd525"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, vocabulary_size, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocabulary_size, 128)\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(256, 512, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool1d(kernel_size=2))\n",
        "        self.fc_layer = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, input_ids_tensor, attention_mask_tensor=None):\n",
        "        embedded_x = self.embedding_layer(input_ids_tensor);\n",
        "        if attention_mask_tensor is not None: embedded_x = embedded_x * attention_mask_tensor.unsqueeze(-1)\n",
        "        permuted_x = embedded_x.permute(0,2,1); convolved_x = self.cnn_layers(permuted_x)\n",
        "        pooled_x = F.adaptive_avg_pool1d(convolved_x,1).squeeze(-1); logits = self.fc_layer(pooled_x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ac2ff9-8056-4f69-b2e4-16dfb361c972",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:36.430524Z",
          "iopub.status.busy": "2025-05-07T07:34:36.430203Z",
          "iopub.status.idle": "2025-05-07T07:34:36.448219Z",
          "shell.execute_reply": "2025-05-07T07:34:36.447664Z",
          "shell.execute_reply.started": "2025-05-07T07:34:36.430505Z"
        },
        "tags": [],
        "id": "f9ac2ff9-8056-4f69-b2e4-16dfb361c972"
      },
      "outputs": [],
      "source": [
        "class StyleClassifier(nn.Module):\n",
        "    def __init__(self, classifier_model_name_str):\n",
        "        super().__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(classifier_model_name_str)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids_tensor, attention_mask_tensor):\n",
        "        bert_output = self.bert(input_ids=input_ids_tensor, attention_mask=attention_mask_tensor)\n",
        "        cls_token_embedding = bert_output.last_hidden_state[:,0,:]\n",
        "        logits = self.classifier(cls_token_embedding)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408988dc-9d9f-47ab-ae82-32bdf4bb98ee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:34:36.449261Z",
          "iopub.status.busy": "2025-05-07T07:34:36.448881Z",
          "iopub.status.idle": "2025-05-07T07:36:19.481709Z",
          "shell.execute_reply": "2025-05-07T07:36:19.480801Z",
          "shell.execute_reply.started": "2025-05-07T07:34:36.449240Z"
        },
        "tags": [],
        "id": "408988dc-9d9f-47ab-ae82-32bdf4bb98ee",
        "outputId": "bcb5cca1-96b4-48f2-918f-a6d41acaec20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Генератор инициализирован: forced_bos=24228, dec_start=24228\n",
            "Классификатор стиля загружен.\n"
          ]
        }
      ],
      "source": [
        "G_model = Generator(GEN_MODEL_NAME, len(gen_tokenizer), RUSSIAN_TOKEN_ID).to(device)\n",
        "D_C_model = Discriminator(len(gen_tokenizer), MAX_LENGTH).to(device)\n",
        "D_L_model = Discriminator(len(gen_tokenizer), MAX_LENGTH).to(device)\n",
        "\n",
        "try:\n",
        "    style_classifier_main_model = StyleClassifier(CLASSIFIER_MODEL_NAME).to(device)\n",
        "    style_classifier_model_path = \"style_classifier/model.pth\";\n",
        "    if not os.path.exists(style_classifier_model_path): raise FileNotFoundError(f\"Нет классификатора: {style_classifier_model_path}\")\n",
        "    style_classifier_main_model.load_state_dict(torch.load(style_classifier_model_path, map_location=device)); print(\"Классификатор стиля загружен.\")\n",
        "except Exception as e: print(f\"Ошибка StyleClassifier: {e}\"); raise\n",
        "style_classifier_main_model.eval(); [param.requires_grad_(False) for param in style_classifier_main_model.parameters()]\n",
        "\n",
        "gen_config_train = update_generation_config(base_gen_config_train, G_model.generation_config_internal, gen_tokenizer.pad_token_id)\n",
        "gen_config_val_log = update_generation_config(base_gen_config_val_log, G_model.generation_config_internal, gen_tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e235d8da-8efb-40b2-943e-51c0f601d530",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:36:19.483305Z",
          "iopub.status.busy": "2025-05-07T07:36:19.482714Z",
          "iopub.status.idle": "2025-05-07T07:36:19.496822Z",
          "shell.execute_reply": "2025-05-07T07:36:19.496008Z",
          "shell.execute_reply.started": "2025-05-07T07:36:19.483257Z"
        },
        "tags": [],
        "id": "e235d8da-8efb-40b2-943e-51c0f601d530"
      },
      "outputs": [],
      "source": [
        "optimizer_G = AdamW(G_model.parameters(), lr=LEARNING_RATE_GEN, eps=1e-8)\n",
        "optimizer_D_C = AdamW(D_C_model.parameters(), lr=LEARNING_RATE_DISC, eps=1e-8)\n",
        "optimizer_D_L = AdamW(D_L_model.parameters(), lr=LEARNING_RATE_DISC, eps=1e-8)\n",
        "\n",
        "num_total_training_steps = NUM_EPOCHS * STEPS_PER_EPOCH; num_warmup_steps_sched = int(0.05 * num_total_training_steps)\n",
        "\n",
        "scheduler_G = get_linear_schedule_with_warmup(optimizer_G, num_warmup_steps=num_warmup_steps_sched, num_training_steps=num_total_training_steps)\n",
        "scheduler_D_C = get_linear_schedule_with_warmup(optimizer_D_C, num_warmup_steps=num_warmup_steps_sched, num_training_steps=num_total_training_steps)\n",
        "scheduler_D_L = get_linear_schedule_with_warmup(optimizer_D_L, num_warmup_steps=num_warmup_steps_sched, num_training_steps=num_total_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "598a5b1b-e23d-4eaf-b66e-cfa64d168d38",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:36:19.498025Z",
          "iopub.status.busy": "2025-05-07T07:36:19.497690Z",
          "iopub.status.idle": "2025-05-07T07:36:19.518886Z",
          "shell.execute_reply": "2025-05-07T07:36:19.518094Z",
          "shell.execute_reply.started": "2025-05-07T07:36:19.498002Z"
        },
        "tags": [],
        "id": "598a5b1b-e23d-4eaf-b66e-cfa64d168d38"
      },
      "outputs": [],
      "source": [
        "grad_scaler = GradScaler()\n",
        "adversarial_loss_fn = nn.BCEWithLogitsLoss()\n",
        "cross_entropy_loss_fn_for_G = nn.CrossEntropyLoss(ignore_index=gen_tokenizer.pad_token_id)\n",
        "style_classification_loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5774c4b-1028-455c-a5e2-e7ced3489c68",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:36:19.520682Z",
          "iopub.status.busy": "2025-05-07T07:36:19.519679Z",
          "iopub.status.idle": "2025-05-07T07:36:19.530774Z",
          "shell.execute_reply": "2025-05-07T07:36:19.529999Z",
          "shell.execute_reply.started": "2025-05-07T07:36:19.520652Z"
        },
        "tags": [],
        "id": "b5774c4b-1028-455c-a5e2-e7ced3489c68"
      },
      "outputs": [],
      "source": [
        "def tokenize_for_style_classifier_utility(texts_batch, style_cls_tokenizer, max_len_val, current_dev):\n",
        "    return style_cls_tokenizer(texts_batch, padding=\"max_length\", truncation=True, max_length=max_len_val, return_tensors='pt').to(current_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402c8a54-ce62-4f92-973d-1f4ad2ee3acd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T07:36:19.532035Z",
          "iopub.status.busy": "2025-05-07T07:36:19.531606Z",
          "iopub.status.idle": "2025-05-07T07:36:19.558032Z",
          "shell.execute_reply": "2025-05-07T07:36:19.557252Z",
          "shell.execute_reply.started": "2025-05-07T07:36:19.532014Z"
        },
        "tags": [],
        "id": "402c8a54-ce62-4f92-973d-1f4ad2ee3acd"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def run_validation_final(\n",
        "    generator_to_validate,\n",
        "    val_dl_C_for_id, val_dl_L_for_id,\n",
        "    val_dl_C_for_style_acc, val_dl_L_for_style_acc,\n",
        "    style_classifier_for_val,\n",
        "    current_validation_gen_config,\n",
        "    num_examples_for_style_eval, current_eval_device\n",
        "):\n",
        "    generator_to_validate.eval()\n",
        "    val_epoch_metrics = {}\n",
        "\n",
        "    current_id_loss_C, num_id_batches_C = 0.0, 0\n",
        "    for batch_data in tqdm(val_dl_C_for_id, desc=\"Validating Identity C->C\", leave=False, ncols=100):\n",
        "        input_ids = batch_data['input_ids'].to(current_eval_device)\n",
        "        attention_mask = batch_data['attention_mask'].to(current_eval_device)\n",
        "        labels = batch_data['labels'].to(current_eval_device)\n",
        "        with autocast():\n",
        "            outputs = generator_to_validate(input_ids, attention_mask, labels=labels)\n",
        "            current_id_loss_C += outputs.loss.item()\n",
        "        num_id_batches_C += 1\n",
        "    val_epoch_metrics['id_loss_C_val'] = current_id_loss_C / num_id_batches_C if num_id_batches_C > 0 else float('inf')\n",
        "\n",
        "    current_id_loss_L, num_id_batches_L = 0.0, 0\n",
        "    for batch_data in tqdm(val_dl_L_for_id, desc=\"Validating Identity L->L\", leave=False, ncols=100):\n",
        "        input_ids = batch_data['input_ids'].to(current_eval_device)\n",
        "        attention_mask = batch_data['attention_mask'].to(current_eval_device)\n",
        "        labels = batch_data['labels'].to(current_eval_device)\n",
        "        with autocast():\n",
        "            outputs = generator_to_validate(input_ids, attention_mask, labels=labels)\n",
        "            current_id_loss_L += outputs.loss.item()\n",
        "        num_id_batches_L += 1\n",
        "    val_epoch_metrics['id_loss_L_val'] = current_id_loss_L / num_id_batches_L if num_id_batches_L > 0 else float('inf')\n",
        "\n",
        "    generated_L_examples_val, total_s_loss_C2L, correct_s_preds_C2L, count_s_C2L = [], 0.0, 0, 0\n",
        "    num_batches_to_process_C = min(len(val_dl_C_for_style_acc), (num_examples_for_style_eval + BATCH_SIZE - 1) // BATCH_SIZE)\n",
        "\n",
        "    for batch_data in tqdm(itertools.islice(val_dl_C_for_style_acc, num_batches_to_process_C), desc=\"Validating Style C->L\", leave=False, ncols=100, total=num_batches_to_process_C):\n",
        "        real_C_text_strings = batch_data['original_text_str']\n",
        "        g_input_val_texts = [f\"{TAG_TO_LIT} {text}\" for text in real_C_text_strings]\n",
        "        tokenized_g_val_input = gen_tokenizer(g_input_val_texts, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(current_eval_device)\n",
        "        with autocast():\n",
        "            fake_L_ids_generated = generator_to_validate.generate_texts(tokenized_g_val_input.input_ids, tokenized_g_val_input.attention_mask, external_generation_config=current_validation_gen_config)\n",
        "        fake_L_text_strings = gen_tokenizer.batch_decode(fake_L_ids_generated, skip_special_tokens=True)\n",
        "        tokenized_for_style_input = tokenize_for_style_classifier_utility(fake_L_text_strings, style_tokenizer, MAX_LENGTH, current_eval_device)\n",
        "        with autocast():\n",
        "            style_logits_output = style_classifier_for_val(tokenized_for_style_input.input_ids, tokenized_for_style_input.attention_mask)\n",
        "        target_style_labels = torch.ones(style_logits_output.size(0), dtype=torch.long, device=current_eval_device)\n",
        "        total_s_loss_C2L += style_classification_loss_fn(style_logits_output, target_style_labels).item() * style_logits_output.size(0)\n",
        "        predicted_style_labels = torch.argmax(style_logits_output, dim=1)\n",
        "        correct_s_preds_C2L += (predicted_style_labels == target_style_labels).sum().item()\n",
        "        count_s_C2L += style_logits_output.size(0)\n",
        "        if not generated_L_examples_val: generated_L_examples_val.extend(list(zip(real_C_text_strings[:3], fake_L_text_strings[:3])))\n",
        "    val_epoch_metrics['style_loss_C2L_val'] = total_s_loss_C2L / count_s_C2L if count_s_C2L > 0 else float('inf')\n",
        "    val_epoch_metrics['style_acc_C2L_val'] = correct_s_preds_C2L / count_s_C2L if count_s_C2L > 0 else 0.0\n",
        "    val_epoch_metrics['example_C2L_gen_val'] = generated_L_examples_val\n",
        "\n",
        "    generated_C_examples_val, total_s_loss_L2C, correct_s_preds_L2C, count_s_L2C = [], 0.0, 0, 0\n",
        "    num_batches_to_process_L = min(len(val_dl_L_for_style_acc), (num_examples_for_style_eval + BATCH_SIZE - 1) // BATCH_SIZE)\n",
        "    for batch_data in tqdm(itertools.islice(val_dl_L_for_style_acc, num_batches_to_process_L), desc=\"Validating Style L->C\", leave=False, ncols=100, total=num_batches_to_process_L):\n",
        "        real_L_text_strings = batch_data['original_text_str']\n",
        "        g_input_val_texts = [f\"{TAG_TO_CONV} {text}\" for text in real_L_text_strings]\n",
        "        tokenized_g_val_input = gen_tokenizer(g_input_val_texts, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(current_eval_device)\n",
        "        with autocast():\n",
        "            fake_C_ids_generated = generator_to_validate.generate_texts(tokenized_g_val_input.input_ids, tokenized_g_val_input.attention_mask, external_generation_config=current_validation_gen_config)\n",
        "        fake_C_text_strings = gen_tokenizer.batch_decode(fake_C_ids_generated, skip_special_tokens=True)\n",
        "        tokenized_for_style_input = tokenize_for_style_classifier_utility(fake_C_text_strings, style_tokenizer, MAX_LENGTH, current_eval_device)\n",
        "        with autocast():\n",
        "            style_logits_output = style_classifier_for_val(tokenized_for_style_input.input_ids, tokenized_for_style_input.attention_mask)\n",
        "        target_style_labels = torch.zeros(style_logits_output.size(0), dtype=torch.long, device=current_eval_device)\n",
        "        total_s_loss_L2C += style_classification_loss_fn(style_logits_output, target_style_labels).item() * style_logits_output.size(0)\n",
        "        predicted_style_labels = torch.argmax(style_logits_output, dim=1)\n",
        "        correct_s_preds_L2C += (predicted_style_labels == target_style_labels).sum().item()\n",
        "        count_s_L2C += style_logits_output.size(0)\n",
        "        if not generated_C_examples_val: generated_C_examples_val.extend(list(zip(real_L_text_strings[:3], fake_C_text_strings[:3])))\n",
        "    val_epoch_metrics['style_loss_L2C_val'] = total_s_loss_L2C / count_s_L2C if count_s_L2C > 0 else float('inf')\n",
        "    val_epoch_metrics['style_acc_L2C_val'] = correct_s_preds_L2C / count_s_L2C if count_s_L2C > 0 else 0.0\n",
        "    val_epoch_metrics['example_L2C_gen_val'] = generated_C_examples_val\n",
        "\n",
        "    val_epoch_metrics['G_total_val_comparable'] = \\\n",
        "        (val_epoch_metrics['id_loss_C_val'] + val_epoch_metrics['id_loss_L_val']) * 0.5 * LAMBDA_IDENTITY + \\\n",
        "        (val_epoch_metrics['style_loss_C2L_val'] + val_epoch_metrics['style_loss_L2C_val']) * 0.5 * LAMBDA_STYLE\n",
        "\n",
        "    generator_to_validate.train()\n",
        "    return val_epoch_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68119f3d-b067-4082-bb34-28268054edc8",
      "metadata": {
        "tags": [],
        "id": "68119f3d-b067-4082-bb34-28268054edc8",
        "outputId": "d879c974-564f-4fb8-95dd-033f48914e79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Эпоха 6/12: 100%|█████████████████████████| 125/125 [31:15<00:00, 15.00s/it, G_Full=151.08, D_Total=1.63, LR_G=1.05e-05]\n",
            "                                                                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Результаты Эпохи 6/12 ---\n",
            "  Потери Тренировки: G_Full=153.002 (Adv=0.779, Style=141.091, Cyc=6.091, Id=5.042), D_Total=1.703\n",
            "                     G_Comparable_Train=73.066\n",
            "  Метрики Валидации: G_Id(C|L)=0.632|0.637, G_Style(C2L L|A)=9.148|3.12%, G_Style(L2C L|A)=5.418|13.54%\n",
            "                     G_Total_Comparable_Val=89.936\n",
            "  Примеры генерации (Валидация):\n",
            "    C->L 1 IN:  \"За некоторыми исключениями ...\"\n",
            "            OUT: \"За некоторыми исключениями ...\"\n",
            "    C->L 2 IN:  \"только батарею жрет и данные пиздит...\"\n",
            "            OUT: \"только батарею жрет и данные пиздит...\"\n",
            "    C->L 3 IN:  \"Чарльз Буковски — Музыка горячей воды                                 ...\"\n",
            "            OUT: \"Чарльз Буковски — Музыка горячей воды :....\"\n",
            "    L->C 1 IN:  \"Я вам не соперница....\"\n",
            "            OUT: \"Я вам не соперница....\"\n",
            "    L->C 2 IN:  \"Но сказанного не воротишь....\"\n",
            "            OUT: \"Но сказанного не воротишь....\"\n",
            "    L->C 3 IN:  \"Ее душа, ее плоть жили одной потребностью любви, всепоглощающей, беско...\"\n",
            "            OUT: \"Ее душа, ее плоть жили одной потребностью любви, всепоглощающей, беско...\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Графики сохранены по пути: tag_cyclegan_bart_final_v2/training_plots_final.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Эпоха 7/12:  21%|█████▍                    | 26/125 [06:19<24:01, 14.56s/it, G_Full=153.96, D_Total=1.84, LR_G=1.02e-05]"
          ]
        }
      ],
      "source": [
        "training_history = {\n",
        "    'epoch': [],\n",
        "    'G_total_train_full': [], 'G_adv_train': [], 'G_cycle_train': [],\n",
        "    'G_identity_train': [], 'G_style_train': [], 'D_total_train': [],\n",
        "    'G_total_train_comparable': [],\n",
        "    'G_id_loss_C_val': [], 'G_id_loss_L_val': [],\n",
        "    'G_style_loss_C2L_val': [], 'G_style_acc_C2L_val': [],\n",
        "    'G_style_loss_L2C_val': [], 'G_style_acc_L2C_val': [],\n",
        "    'G_total_val_comparable': []\n",
        "}\n",
        "best_validation_metric = float('inf')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "data_iterator_C = itertools.cycle(train_dataloader_C)\n",
        "data_iterator_L = itertools.cycle(train_dataloader_L)\n",
        "\n",
        "for epoch_num in range(NUM_EPOCHS):\n",
        "    G_model.train(); D_C_model.train(); D_L_model.train()\n",
        "\n",
        "    current_epoch_train_losses_sum = {\n",
        "        'G_total_train_full': 0.0, 'G_adv_train': 0.0, 'G_cycle_train': 0.0,\n",
        "        'G_identity_train': 0.0, 'G_style_train': 0.0, 'D_total_train': 0.0,\n",
        "        'G_total_train_comparable': 0.0\n",
        "    }\n",
        "\n",
        "    progress_bar = tqdm(range(STEPS_PER_EPOCH), desc=f\"Эпоха {epoch_num + 1}/{NUM_EPOCHS}\", ncols=120)\n",
        "\n",
        "    for step_num in progress_bar:\n",
        "        batch_C_data = next(data_iterator_C)\n",
        "        batch_L_data = next(data_iterator_L)\n",
        "\n",
        "        real_C_original_ids = batch_C_data['original_ids'].to(device)\n",
        "        real_C_original_mask = batch_C_data['original_mask'].to(device)\n",
        "        real_C_original_texts_list = batch_C_data['original_text_str']\n",
        "\n",
        "        real_L_original_ids = batch_L_data['original_ids'].to(device)\n",
        "        real_L_original_mask = batch_L_data['original_mask'].to(device)\n",
        "        real_L_original_texts_list = batch_L_data['original_text_str']\n",
        "\n",
        "        optimizer_D_C.zero_grad()\n",
        "        optimizer_D_L.zero_grad()\n",
        "\n",
        "        g_input_C_to_L_texts_list = [f\"{TAG_TO_LIT} {text}\" for text in real_C_original_texts_list]\n",
        "        tokenized_g_input_C_to_L = gen_tokenizer(g_input_C_to_L_texts_list, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(device)\n",
        "\n",
        "        g_input_L_to_C_texts_list = [f\"{TAG_TO_CONV} {text}\" for text in real_L_original_texts_list]\n",
        "        tokenized_g_input_L_to_C = gen_tokenizer(g_input_L_to_C_texts_list, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with autocast():\n",
        "                fake_L_generated_ids = G_model.generate_texts(tokenized_g_input_C_to_L.input_ids, tokenized_g_input_C_to_L.attention_mask, external_generation_config=gen_config_train)\n",
        "                fake_C_generated_ids = G_model.generate_texts(tokenized_g_input_L_to_C.input_ids, tokenized_g_input_L_to_C.attention_mask, external_generation_config=gen_config_train)\n",
        "\n",
        "        fake_L_generated_mask = G_model.get_attention_mask_for_generated(fake_L_generated_ids)\n",
        "        fake_C_generated_mask = G_model.get_attention_mask_for_generated(fake_C_generated_ids)\n",
        "\n",
        "        with autocast():\n",
        "            d_l_pred_on_real = D_L_model(real_L_original_ids, real_L_original_mask)\n",
        "            d_l_pred_on_fake = D_L_model(fake_L_generated_ids.detach(), fake_L_generated_mask)\n",
        "            loss_D_L_total_step = (adversarial_loss_fn(d_l_pred_on_real, torch.ones_like(d_l_pred_on_real)) + \\\n",
        "                                   adversarial_loss_fn(d_l_pred_on_fake, torch.zeros_like(d_l_pred_on_fake))) * 0.5\n",
        "\n",
        "            d_c_pred_on_real = D_C_model(real_C_original_ids, real_C_original_mask)\n",
        "            d_c_pred_on_fake = D_C_model(fake_C_generated_ids.detach(), fake_C_generated_mask)\n",
        "            loss_D_C_total_step = (adversarial_loss_fn(d_c_pred_on_real, torch.ones_like(d_c_pred_on_real)) + \\\n",
        "                                   adversarial_loss_fn(d_c_pred_on_fake, torch.zeros_like(d_c_pred_on_fake))) * 0.5\n",
        "\n",
        "            loss_D_combined_step = loss_D_L_total_step + loss_D_C_total_step\n",
        "\n",
        "        grad_scaler.scale(loss_D_combined_step).backward()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        with autocast():\n",
        "            fake_L_ids_for_G = G_model.generate_texts(tokenized_g_input_C_to_L.input_ids, tokenized_g_input_C_to_L.attention_mask, external_generation_config=gen_config_train)\n",
        "            fake_C_ids_for_G = G_model.generate_texts(tokenized_g_input_L_to_C.input_ids, tokenized_g_input_L_to_C.attention_mask, external_generation_config=gen_config_train)\n",
        "            fake_L_mask_for_G = G_model.get_attention_mask_for_generated(fake_L_ids_for_G)\n",
        "            fake_C_mask_for_G = G_model.get_attention_mask_for_generated(fake_C_ids_for_G)\n",
        "\n",
        "            loss_G_adv_L_component = adversarial_loss_fn(D_L_model(fake_L_ids_for_G, fake_L_mask_for_G), torch.ones_like(D_L_model(fake_L_ids_for_G, fake_L_mask_for_G)))\n",
        "            loss_G_adv_C_component = adversarial_loss_fn(D_C_model(fake_C_ids_for_G, fake_C_mask_for_G), torch.ones_like(D_C_model(fake_C_ids_for_G, fake_C_mask_for_G)))\n",
        "            loss_G_adversarial_total = (loss_G_adv_L_component + loss_G_adv_C_component) * LAMBDA_ADV\n",
        "\n",
        "            fake_L_texts_for_style_clf = gen_tokenizer.batch_decode(fake_L_ids_for_G, skip_special_tokens=True)\n",
        "            fake_C_texts_for_style_clf = gen_tokenizer.batch_decode(fake_C_ids_for_G, skip_special_tokens=True)\n",
        "            tokenized_L_for_style = tokenize_for_style_classifier_utility(fake_L_texts_for_style_clf, style_tokenizer, MAX_LENGTH, device)\n",
        "            tokenized_C_for_style = tokenize_for_style_classifier_utility(fake_C_texts_for_style_clf, style_tokenizer, MAX_LENGTH, device)\n",
        "            style_L_predictions = style_classifier_main_model(tokenized_L_for_style.input_ids, tokenized_L_for_style.attention_mask)\n",
        "            style_C_predictions = style_classifier_main_model(tokenized_C_for_style.input_ids, tokenized_C_for_style.attention_mask)\n",
        "            loss_G_style_L_comp = style_classification_loss_fn(style_L_predictions, torch.ones(style_L_predictions.size(0), dtype=torch.long, device=device))\n",
        "            loss_G_style_C_comp = style_classification_loss_fn(style_C_predictions, torch.zeros(style_C_predictions.size(0), dtype=torch.long, device=device))\n",
        "            loss_G_style_total = (loss_G_style_L_comp + loss_G_style_C_comp) * LAMBDA_STYLE\n",
        "\n",
        "            g_input_reconstruct_C_texts_list = [f\"{TAG_TO_CONV} {text}\" for text in fake_L_texts_for_style_clf]\n",
        "            tokenized_g_input_reconstruct_C = gen_tokenizer(g_input_reconstruct_C_texts_list, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(device)\n",
        "            reconstructed_C_outputs = G_model(tokenized_g_input_reconstruct_C.input_ids, tokenized_g_input_reconstruct_C.attention_mask, labels=real_C_original_ids)\n",
        "            loss_G_cycle_C_component = reconstructed_C_outputs.loss\n",
        "            g_input_reconstruct_L_texts_list = [f\"{TAG_TO_LIT} {text}\" for text in fake_C_texts_for_style_clf]\n",
        "            tokenized_g_input_reconstruct_L = gen_tokenizer(g_input_reconstruct_L_texts_list, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(device)\n",
        "            reconstructed_L_outputs = G_model(tokenized_g_input_reconstruct_L.input_ids, tokenized_g_input_reconstruct_L.attention_mask, labels=real_L_original_ids)\n",
        "            loss_G_cycle_L_component = reconstructed_L_outputs.loss\n",
        "            loss_G_cycle_total = (loss_G_cycle_C_component + loss_G_cycle_L_component) * LAMBDA_CYCLE\n",
        "\n",
        "            g_input_identity_L_texts_list = [f\"{TAG_TO_LIT} {text}\" for text in real_L_original_texts_list]\n",
        "            tokenized_g_input_identity_L = gen_tokenizer(g_input_identity_L_texts_list, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(device)\n",
        "            identity_L_outputs = G_model(tokenized_g_input_identity_L.input_ids, tokenized_g_input_identity_L.attention_mask, labels=real_L_original_ids)\n",
        "            loss_G_identity_L_component = identity_L_outputs.loss\n",
        "            g_input_identity_C_texts_list = [f\"{TAG_TO_CONV} {text}\" for text in real_C_original_texts_list]\n",
        "            tokenized_g_input_identity_C = gen_tokenizer(g_input_identity_C_texts_list, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors='pt').to(device)\n",
        "            identity_C_outputs = G_model(tokenized_g_input_identity_C.input_ids, tokenized_g_input_identity_C.attention_mask, labels=real_C_original_ids)\n",
        "            loss_G_identity_C_component = identity_C_outputs.loss\n",
        "            loss_G_identity_total = (loss_G_identity_L_component + loss_G_identity_C_component) * LAMBDA_IDENTITY\n",
        "\n",
        "            loss_G_full_step = loss_G_adversarial_total + loss_G_style_total + loss_G_cycle_total + loss_G_identity_total\n",
        "\n",
        "        grad_scaler.scale(loss_G_full_step).backward()\n",
        "\n",
        "        grad_scaler.step(optimizer_D_L)\n",
        "        grad_scaler.step(optimizer_D_C)\n",
        "        torch.nn.utils.clip_grad_norm_(G_model.parameters(), 1.0)\n",
        "        grad_scaler.step(optimizer_G)\n",
        "\n",
        "        grad_scaler.update()\n",
        "\n",
        "        scheduler_G.step()\n",
        "        scheduler_D_C.step()\n",
        "        scheduler_D_L.step()\n",
        "\n",
        "        current_epoch_train_losses_sum['D_total_train'] += loss_D_combined_step.item()\n",
        "        current_epoch_train_losses_sum['G_total_train_full'] += loss_G_full_step.item()\n",
        "        current_epoch_train_losses_sum['G_adv_train'] += loss_G_adversarial_total.item()\n",
        "        current_epoch_train_losses_sum['G_style_train'] += loss_G_style_total.item()\n",
        "        current_epoch_train_losses_sum['G_cycle_train'] += loss_G_cycle_total.item()\n",
        "        current_epoch_train_losses_sum['G_identity_train'] += loss_G_identity_total.item()\n",
        "\n",
        "        g_total_comparable_train_step_unweighted_id = (loss_G_identity_L_component.item() + loss_G_identity_C_component.item()) * 0.5\n",
        "        g_total_comparable_train_step_unweighted_style = (loss_G_style_L_comp.item() + loss_G_style_C_comp.item()) * 0.5\n",
        "        g_total_comparable_train_step = (g_total_comparable_train_step_unweighted_id * LAMBDA_IDENTITY) + \\\n",
        "                                    (g_total_comparable_train_step_unweighted_style * LAMBDA_STYLE)\n",
        "        current_epoch_train_losses_sum['G_total_train_comparable'] += g_total_comparable_train_step\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            \"G_Full\": f\"{loss_G_full_step.item():.2f}\",\n",
        "            \"D_Total\": f\"{loss_D_combined_step.item():.2f}\",\n",
        "            \"LR_G\": f\"{scheduler_G.get_last_lr()[0]:.2e}\"\n",
        "        })\n",
        "\n",
        "    training_history['epoch'].append(epoch_num + 1)\n",
        "    for key_hist in current_epoch_train_losses_sum:\n",
        "        training_history[key_hist].append(current_epoch_train_losses_sum[key_hist] / STEPS_PER_EPOCH)\n",
        "\n",
        "    validation_epoch_results = run_validation_final(\n",
        "        G_model, val_dataloader_C_identity, val_dataloader_L_identity,\n",
        "        val_dataloader_C_for_style, val_dataloader_L_for_style,\n",
        "        style_classifier_main_model,\n",
        "        gen_config_val_log,\n",
        "        NUM_VAL_STYLE_EXAMPLES, device\n",
        "    )\n",
        "\n",
        "    training_history['G_id_loss_C_val'].append(validation_epoch_results['id_loss_C_val'])\n",
        "    training_history['G_id_loss_L_val'].append(validation_epoch_results['id_loss_L_val'])\n",
        "    training_history['G_style_loss_C2L_val'].append(validation_epoch_results['style_loss_C2L_val'])\n",
        "    training_history['G_style_acc_C2L_val'].append(validation_epoch_results['style_acc_C2L_val'])\n",
        "    training_history['G_style_loss_L2C_val'].append(validation_epoch_results['style_loss_L2C_val'])\n",
        "    training_history['G_style_acc_L2C_val'].append(validation_epoch_results['style_acc_L2C_val'])\n",
        "    training_history['G_total_val_comparable'].append(validation_epoch_results['G_total_val_comparable'])\n",
        "\n",
        "    print(f\"\\n--- Результаты Эпохи {epoch_num + 1}/{NUM_EPOCHS} ---\")\n",
        "    print(f\"  Потери Тренировки: G_Full={training_history['G_total_train_full'][-1]:.3f} \"\n",
        "          f\"(Adv={training_history['G_adv_train'][-1]:.3f}, Style={training_history['G_style_train'][-1]:.3f}, \"\n",
        "          f\"Cyc={training_history['G_cycle_train'][-1]:.3f}, Id={training_history['G_identity_train'][-1]:.3f}), \"\n",
        "          f\"D_Total={training_history['D_total_train'][-1]:.3f}\")\n",
        "    print(f\"                     G_Comparable_Train={training_history['G_total_train_comparable'][-1]:.3f}\")\n",
        "    print(f\"  Метрики Валидации: G_Id(C|L)={validation_epoch_results['id_loss_C_val']:.3f}|{validation_epoch_results['id_loss_L_val']:.3f}, \"\n",
        "          f\"G_Style(C2L L|A)={validation_epoch_results['style_loss_C2L_val']:.3f}|{validation_epoch_results['style_acc_C2L_val']:.2%}, \"\n",
        "          f\"G_Style(L2C L|A)={validation_epoch_results['style_loss_L2C_val']:.3f}|{validation_epoch_results['style_acc_L2C_val']:.2%}\")\n",
        "    print(f\"                     G_Total_Comparable_Val={validation_epoch_results['G_total_val_comparable']:.3f}\")\n",
        "\n",
        "    print(\"  Примеры генерации (Валидация):\")\n",
        "    for i, (inp_text, gen_text) in enumerate(validation_epoch_results['example_C2L_gen_val']):\n",
        "        print(f\"    C->L {i+1} IN:  \\\"{inp_text[:70].replace(chr(10),' ')}...\\\"\")\n",
        "        print(f\"            OUT: \\\"{gen_text[:70].replace(chr(10),' ')}...\\\"\")\n",
        "    for i, (inp_text, gen_text) in enumerate(validation_epoch_results['example_L2C_gen_val']):\n",
        "        print(f\"    L->C {i+1} IN:  \\\"{inp_text[:70].replace(chr(10),' ')}...\\\"\")\n",
        "        print(f\"            OUT: \\\"{gen_text[:70].replace(chr(10),' ')}...\\\"\")\n",
        "\n",
        "    current_validation_metric_for_saving = validation_epoch_results['G_total_val_comparable']\n",
        "    if current_validation_metric_for_saving < best_validation_metric:\n",
        "        best_validation_metric = current_validation_metric_for_saving\n",
        "        torch.save(G_model.state_dict(), MODEL_PATH_G_BEST)\n",
        "        torch.save(D_C_model.state_dict(), MODEL_PATH_D_C_BEST)\n",
        "        torch.save(D_L_model.state_dict(), MODEL_PATH_D_L_BEST)\n",
        "        print(f\"  *** Новая лучшая модель сохранена! Val G_Comparable_Loss: {best_validation_metric:.3f} ***\")\n",
        "\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(14, 21))\n",
        "    fig.suptitle(f\"Результаты обучения - Эпоха {epoch_num + 1}\", fontsize=16)\n",
        "\n",
        "    axs[0].plot(training_history['epoch'], training_history['G_total_train_full'], '-o', label='G Total Train (Full)', linewidth=2)\n",
        "    axs[0].plot(training_history['epoch'], training_history['G_adv_train'], ':o', label='G Adv Train')\n",
        "    axs[0].plot(training_history['epoch'], training_history['G_style_train'], ':o', label='G Style Train')\n",
        "    axs[0].plot(training_history['epoch'], training_history['G_cycle_train'], ':o', label='G Cycle Train')\n",
        "    axs[0].plot(training_history['epoch'], training_history['G_identity_train'], ':o', label='G Identity Train')\n",
        "    axs[0].plot(training_history['epoch'], training_history['D_total_train'], '-x', label='D Total Train', linewidth=2)\n",
        "    axs[0].set_title('Тренировочные Потери (Все Компоненты G)'); axs[0].set_xlabel('Эпоха'); axs[0].set_ylabel('Потеря')\n",
        "    axs[0].legend(loc='upper right'); axs[0].grid(True)\n",
        "\n",
        "    ax1_val_loss = axs[1]; ax1_val_acc = axs[1].twinx()\n",
        "    p1, = ax1_val_loss.plot(training_history['epoch'], training_history['G_id_loss_C_val'], '-o', label='G Id_C Val', color='royalblue')\n",
        "    p2, = ax1_val_loss.plot(training_history['epoch'], training_history['G_id_loss_L_val'], '-o', label='G Id_L Val', color='darkorange')\n",
        "    p3, = ax1_val_loss.plot(training_history['epoch'], training_history['G_style_loss_C2L_val'], '--o', label='G Style C->L Loss Val', color='forestgreen')\n",
        "    p4, = ax1_val_loss.plot(training_history['epoch'], training_history['G_style_loss_L2C_val'], '--o', label='G Style L->C Loss Val', color='crimson')\n",
        "    p5, = ax1_val_acc.plot(training_history['epoch'], training_history['G_style_acc_C2L_val'], '-x', label='G Style C->L Acc Val', color='lime')\n",
        "    p6, = ax1_val_acc.plot(training_history['epoch'], training_history['G_style_acc_L2C_val'], '-x', label='G Style L->C Acc Val', color='cyan')\n",
        "    ax1_val_loss.set_title('Валидационные Метрики Генератора'); ax1_val_loss.set_xlabel('Эпоха'); ax1_val_loss.set_ylabel('Потеря (Loss)')\n",
        "    ax1_val_acc.set_ylabel('Точность (Accuracy)', color='teal'); ax1_val_acc.tick_params(axis='y', labelcolor='teal')\n",
        "    handles1, labels1 = ax1_val_loss.get_legend_handles_labels(); handles2, labels2 = ax1_val_acc.get_legend_handles_labels()\n",
        "    ax1_val_loss.legend(handles=handles1 + handles2, labels=labels1 + labels2, loc='center left', bbox_to_anchor=(0.05, 0.5))\n",
        "    ax1_val_loss.grid(True)\n",
        "\n",
        "    axs[2].plot(training_history['epoch'], training_history['G_total_train_comparable'], '-o', label='G Total Comparable Train (Id+Style)')\n",
        "    axs[2].plot(training_history['epoch'], training_history['G_total_val_comparable'], '-x', label='G Total Comparable Val (Id+Style)')\n",
        "    axs[2].set_title('Сравнение G Total Comparable Loss (Train vs Val)'); axs[2].set_xlabel('Эпоха'); axs[2].set_ylabel('Потеря (Weighted Id+Style)')\n",
        "    axs[2].legend(loc='upper right'); axs[2].grid(True)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.savefig(PLOT_PATH); plt.close(fig)\n",
        "    print(f\"Графики сохранены по пути: {PLOT_PATH}\")\n",
        "\n",
        "print(\"--- Обучение завершено ---\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DataSphere Kernel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}